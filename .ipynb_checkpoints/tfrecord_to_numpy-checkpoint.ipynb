{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts TFRecord data into numpy. Feel free to modify based on your needs.\n",
    "\n",
    "Data is saved into pickle files. Every file contains a list of samples. # of the samples in a file can be set via config['num_samples_in_numpy_list']. \n",
    "\n",
    "Loading:\n",
    "data = pickle.load(open(<i>path-to-pkl-file</i>, 'rb'))\n",
    "\n",
    "Each sample is a dictionary with the following fields:\n",
    "<ol>\n",
    "  <li>'label': label of the gesture. A unique ID in {0,1,..,19},</li>\n",
    "  <li>'length': length of the gesture sequence, i.e., # of frames,</li>\n",
    "  <li>'depth': tensor of depth images (length, height, width, 1),</li>\n",
    "  <li>'skeleton': tensor of skeleton joints (length, 180),</li>\n",
    "  <li>'rgb': tensor of rgb images (length, height, width, 3),</li>\n",
    "  <li>'segmentation': tensor of segmentation masks (length, height, width, 3).</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Note that samples have different number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMask(img, segmentedUser):\n",
    "    \"\"\"\n",
    "    Applies mask on the given image for visualization.\n",
    "    \"\"\"\n",
    "    if len(img.shape) > 2: # Color image\n",
    "        mask3 = segmentedUser > 150\n",
    "        masked_img = img * mask3\n",
    "    else:\n",
    "        mask2 = np.mean(segmentedUser, axis=2) > 150\n",
    "        masked_img = img * mask2\n",
    "    return masked_img\n",
    "\n",
    "def read_and_decode_sequence(filename_queue, config):\n",
    "    # Create a TFRecordReader.\n",
    "    readerOptions = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    reader = tf.TFRecordReader(options=readerOptions)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    # Read one sequence sample.\n",
    "    # The training and validation files contains the following fields:\n",
    "    # - label: label of the sequence which take values between 1 and 20.\n",
    "    # - length: length of the sequence, i.e., number of frames.\n",
    "    # - depth: sequence of depth images. [length x height x width x numChannels]\n",
    "    # - rgb: sequence of rgb images. [length x height x width x numChannels]\n",
    "    # - segmentation: sequence of segmentation maskes. [length x height x width x numChannels]\n",
    "    # - skeleton: sequence of flattened skeleton joint positions. [length x numJoints]\n",
    "    #\n",
    "    # The test files doesn't contain \"label\" field.\n",
    "    # [height, width, numChannels] = [80, 80, 3]\n",
    "    with tf.name_scope(\"TFRecordDecoding\"):\n",
    "        context_encoded, sequence_encoded = tf.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                # \"label\" and \"lenght\" are encoded as context features. \n",
    "                context_features={\n",
    "                    \"label\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "                },\n",
    "                # \"depth\", \"rgb\", \"segmentation\", \"skeleton\" are encoded as sequence features.\n",
    "                sequence_features={\n",
    "                    \"depth\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"rgb\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"segmentation\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                    \"skeleton\": tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "                })\n",
    "\n",
    "        # Fetch required data fields. \n",
    "        seq_rgb = tf.decode_raw(sequence_encoded['rgb'], tf.uint8)\n",
    "        seq_depth = tf.decode_raw(sequence_encoded['depth'], tf.uint8)\n",
    "        seq_segmentation = tf.decode_raw(sequence_encoded['segmentation'], tf.uint8)\n",
    "        seq_skeleton = tf.decode_raw(sequence_encoded['skeleton'], tf.float32)\n",
    "        seq_len = tf.to_int32(context_encoded['length'])\n",
    "        seq_label = context_encoded['label']\n",
    "        # Tensorflow requires the labels start from 0. Before you create submission csv,\n",
    "        # increment the predictions by 1.\n",
    "        seq_label = seq_label - 1\n",
    "        \n",
    "        \n",
    "        #[seq_len, num_skeleton_joints]\n",
    "        seq_skeleton = tf.reshape(seq_skeleton, (seq_len, 180))\n",
    "        \n",
    "        # Reshape images.\n",
    "        #[seq_len, height, width, num_channels]\n",
    "        seq_rgb = tf.to_float(tf.reshape(seq_rgb, (-1, config['img_height'], config['img_width'], 3)))\n",
    "        seq_depth = tf.to_float(tf.reshape(seq_depth, (-1, config['img_height'], config['img_width'], 1)))\n",
    "        seq_segmentation = tf.to_float(tf.reshape(seq_segmentation, (-1, config['img_height'], config['img_width'], 3)))\n",
    "    \n",
    "        sample = {}\n",
    "        sample['rgb'] = seq_rgb\n",
    "        sample['depth'] = seq_depth\n",
    "        sample['segmentation'] = seq_segmentation\n",
    "        sample['skeleton'] = seq_skeleton\n",
    "        sample['seq_len'] = seq_len\n",
    "        sample['labels'] = seq_label\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "\n",
    "def input_pipeline(filenames, config):\n",
    "    with tf.name_scope(\"input_pipeline\"):\n",
    "        # Create a queue of TFRecord input files.\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=config['num_epochs'], shuffle=True)\n",
    "        # Read the data from TFRecord files, decode and create a list of data samples by using threads.\n",
    "        sample_list = [read_and_decode_sequence(filename_queue, config) for _ in range(config['ip_num_read_threads'])]\n",
    "        # Create batches.\n",
    "        # Since the data consists of variable-length sequences, allow padding by setting dynamic_pad parameter.\n",
    "        # \"batch_join\" creates batches of samples and pads the sequences w.r.t the max-length sequence in the batch.\n",
    "        # Hence, the padded sequence length can be different for different batches.\n",
    "        batch_sample = tf.train.batch_join(sample_list,\n",
    "                                            batch_size=config['batch_size'],\n",
    "                                            capacity=config['ip_queue_capacity'],\n",
    "                                            enqueue_many=False,\n",
    "                                            dynamic_pad=True,\n",
    "                                            name=\"batch_join_and_pad\")\n",
    "\n",
    "        return batch_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "# TODO: You can change these fields.\n",
    "config['input_dir'] = \"/Users/zhou/Machine_Perception/mp18-dynamic-gesture-recognition/train\" # Directory of the tfrecords.\n",
    "config['input_file_format'] = \"dataTrain_%d.tfrecords\" # File naming\n",
    "config['input_file_ids'] = list(range(1,5)) # File IDs to be used for training.\n",
    "\n",
    "config['num_samples_in_numpy_list'] = 100 # Put 100 samples in a pickle data file. You can put everything in a single file as well.\n",
    "config['output_dir'] = config['input_dir']\n",
    "config['output_file_format'] = config['input_file_format'].split(\".\")[0]+\".pkl\"\n",
    "config['output_file_start_id'] = 1\n",
    "\n",
    "# Keep these fields fixed.\n",
    "config['img_height'] = 80\n",
    "config['img_width'] = 80\n",
    "config['img_num_channels'] = 3\n",
    "config['num_epochs'] = 1\n",
    "config['batch_size'] = 1\n",
    "# Capacity of the queue which contains the samples read by data readers.\n",
    "# Make sure that it has enough capacity.\n",
    "config['ip_queue_capacity'] = config['batch_size']*10  \n",
    "config['ip_num_read_threads'] = 1\n",
    "\n",
    "\n",
    "# Create a list of TFRecord input files.\n",
    "filenames = [os.path.join(config['input_dir'], config['input_file_format'] % i) for i in config['input_file_ids']]\n",
    "\n",
    "# Create data loading operators. This will be represented as a node in the computational graph.\n",
    "# Each <key,value> pair in `batch_sample_dict_op` corresponds to Tensorflow placeholder. Alternatively we could \n",
    "# load data into memory and feed to the model by using feed_dict approach.\n",
    "batch_sample_dict_op = input_pipeline(filenames, config)\n",
    "\n",
    "# Create tensorflow session and initialize the variables (if any).\n",
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "# Create threads to prefetch the data.\n",
    "# https://www.tensorflow.org/programmers_guide/reading_data#creating_threads_to_prefetch_using_queuerunner_objects\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Path to training split./dataTrain_3.tfrecords; No such file or directory\n",
      "\t [[Node: input_pipeline/ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](input_pipeline/TFRecordReaderV2, input_pipeline/input_producer)]]\n",
      "Done.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Path to training split./dataTrain_3.tfrecords; No such file or directory\n\t [[Node: input_pipeline/ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](input_pipeline/TFRecordReaderV2, input_pipeline/input_producer)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d8784b718899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Wait for threads to finish.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mstragglers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_live_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, sess, enqueue_op, coord)\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m           \u001b[0menqueue_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue_closed_exception_types\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=catching-non-exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m           \u001b[0;31m# This exception indicates that a queue was closed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_single_operation_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_single_operation_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_tf_sessionrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_single_operation_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Path to training split./dataTrain_3.tfrecords; No such file or directory\n\t [[Node: input_pipeline/ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](input_pipeline/TFRecordReaderV2, input_pipeline/input_producer)]]"
     ]
    }
   ],
   "source": [
    "np_file_id = config['output_file_start_id']\n",
    "output_list = []\n",
    "num_samples_read = 0\n",
    "try:\n",
    "    while not coord.should_stop():\n",
    "        batch_sample_dict = sess.run(batch_sample_dict_op)\n",
    "        num_samples_read += 1\n",
    "        data_sample = {}\n",
    "        data_sample['rgb'] = batch_sample_dict['rgb'][0] # Data is in batch format. Get rid of the first dimension.\n",
    "        data_sample['depth'] = batch_sample_dict['depth'][0]\n",
    "        data_sample['segmentation'] = batch_sample_dict['segmentation'][0]\n",
    "        data_sample['skeleton'] = batch_sample_dict['skeleton'][0]\n",
    "        data_sample['labels'] = batch_sample_dict['labels'][0]\n",
    "        data_sample['seq_len'] = batch_sample_dict['seq_len'][0]\n",
    "        output_list.append(data_sample)\n",
    "        \n",
    "        if num_samples_read%config['num_samples_in_numpy_list'] == 0:\n",
    "            pickle.dump(output_list, open(os.path.join(config['output_dir'], config['output_file_format'] % np_file_id), 'wb'))\n",
    "            np_file_id += 1\n",
    "            output_list = []\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    # Save last run.\n",
    "    if len(output_list) > 0:\n",
    "        pickle.dump(output_list, open(os.path.join(config['output_dir'], config['output_file_format'] % np_file_id), 'wb'))\n",
    "        output_list = []\n",
    "    print('Done.')\n",
    "finally:\n",
    "    # When done, ask the threads to stop.\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish.\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
